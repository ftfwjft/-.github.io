## Four Generations of High-Dimensional Neural Network Potentials‘



#### Abstract

高维神经网络势(HDNNPs)

- 第一代是由早期为低维系统设计的神经网络势形成的。

- 第二代高维神经网络位势建立的基础是三个关键步骤:第一，将总能量表示为与环境相关的原子能量贡献之和;第二，用原子中心对称函数描述原子环境，作为满足转动、平移和置换不变性要求的描述符;第三，通过主动学习迭代构建参考电子结构数据集。

- 在第三代HDNNPs中，采用了环境依赖的部分电荷，由原子神经网络表示。

- 在第四代HDNNPs中，还可以包括远程电荷转移等非局域现象。

![image-20220212153916479](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212153916479.png)

![image-20220210103327495](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210103327495.png)

势能金字塔的原子模拟举例说明了水和水系统。使用基于函数高层波方法由Ψ只有小系统的几何形状如水在真空访问集群,在密度泛函理论(DFT)是标准的方法来确定简单的属性,如径向分布函数(RDF)从头开始的液态水分子动力学模拟。复杂系统(如电解质或固-液界面)的大规模模拟，或复杂热力学性质的确定，只能使用原子势(如力场)进行。



ML电位可由以下三个标准定义:

1. 一个ML势是势能表面的解析表达式，提供了势能及其解析导数作为一个函数的原子位置使用ML算法。
2. 一个ML势是用一组一致的参考电子结构数据构建的。
3. 除了在所选的参考电子结构方法中隐含的近似外，ML势不包含任何关于功能形式的特别假设。



如果适用以下标准，我们将ML势称为高维的:

- ML势应该适用于非常多的原子，典型的数以万计，同时满足所需的平移，旋转，和置换不变的PES精确。
- ML势应该明确地依赖于系统的所有自由度。



![image-20220210103935000](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210103935000.png)

本文综述了四代神经网络电位的研究进展。第一代NNPs仅适用于低维系统，而含有大量原子的高维系统可以用第二代和后代的势来描述。第二代HDNNPs基于环境依赖的原子能。使用局部电荷的第三代HDNNPs包含了长程静电相互作用，而第四代HDNNPs考虑了包括非局部电荷转移在内的全局电荷分布。



#### FIRST-GENERATION NEURAL NETWORK POTENTIALS



![image-20220210104117278](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104117278.png)

小型前馈神经网络的结构。对于这个二维系统，输出能量E是两个输入坐标G1和G2的函数。k层和E层神经元i的yik值取决于连接权值aijkl和偏置权值bik。

![image-20220210104333040](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104333040.png)



#### SECOND-GENERATION NEURAL NETWORK POTENTIALS



![image-20220210104359726](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104359726.png)

![image-20220210104405091](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104405091.png)

在具有周期性边界条件的系统中，围绕中心原子以红色表示的半径r的截止球的示意图。在截止球内的所有原子，进入中心原子的原子能贡献，显示为橙色。

![image-20220210104426560](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104426560.png)

![image-20220210104508554](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104508554.png)

​													       机器学习势中常用的一些典型描述符列表

许多不同的函数形式可以用来定义截止函数fc(Rij)，

![image-20220210104603797](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104603797.png)

![image-20220210104616740](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104616740.png)

最常用的径向函数类型由截止球内所有原子j的高斯函数和截止函数的乘积和组成:

![image-20220210104647779](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104647779.png)

![image-20220210104657560](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104657560.png)

​																					径向对称functions

也有备选的径向acsf，如:

![image-20220210104736626](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104736626.png)

最常用的有角ACSF有形式:

![image-20220210104800680](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210104800680.png)

![image-20220210105107426](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210105107426.png)

![image-20220210105121559](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210105121559.png)

角对称函数中使用的角项21  - ζ(1 + λ cos θijk)ζ， eq 13。λ = 1的项用蓝色到黑色表示，λ =−1的项用橙色到棕色表示。使用的指数是ζ = 1, 2,  4，和16(从亮到暗色调)。



在ANI-1第二代HDNNP中，是一个应用，Behler-Parrinello方法构建有机分子系统的通用ML势，重点关注可转移性，提出了一种改进的角ACSF，

![image-20220210105347831](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210105347831.png)

其中包括径向和角位移参数θs和r，允许使角函数的峰值适应于任意值。它的参数必须谨慎选择，以保持角对称与0°和180°

![image-20220210105439317](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210105439317.png)

一个考虑到原子i和j的环境的径向点扩展函数的例子是一个简单函数，它是由所有原子k至少在原子i或j的截止球内定义的

![image-20220210105455928](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210105455928.png)

在这里，取决于它的位置，每个原子k可以贡献一个或两个截止项fc(Rik)和fc(Rjk)中心的原子i和j，分别。通过包含一个高斯项，可以类推进行扩展

![image-20220210105517816](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210105517816.png)

到目前为止，acsf代表了构建HDNNPs的标准描述符。虽然它们已经在许多成功的应用中得到了应用，但仍存在一些挑战，需要做出一些决策，这些决策将影响所获得的潜力的性能和准确性。

首先，acsf的空间形状必须通过选择方程中的参数来定义。有几种方法可以指导这些参数的选择，基本上可以区分出两种策略，它们的目标要么是对空间环境进行无偏的描述，要么是对给定数据集进行最好的描述。考虑到在对这对元素的模拟中预期的最小原子间距离，通常通过为每个元素组合选择大约6个径向函数来获得第一组无偏的“默认”初始acsf。

最佳策略取决于特定的情况。在潜在构建的早期阶段，一般的对称函数应该是首选的，它提供了一个无偏的空间覆盖，而对于大的可用数据集，定制的对称函数可能更有效，这通常也是由经验确定的

对于这两种策略，必须避免某些acsf的函数值范围过小。这些功能将不必要地增加ACSF集合的大小，但它们并不有助于结构分化。然而，主要的问题是，在这种情况下，对称函数值中非常小且结构无关的差异可能被NN在训练过程中利用来表示势能。这种非物理行为会导致数值的不稳定性，因此在训练开始之前应该删除这些函数。

用对称函数和许多相关描述符描述原子环境的一个明显限制是，函数的数量与元素的数量呈不利比例，因为所有可能的元素排列都必须包含在内。

![image-20220210105811883](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210105811883.png)

其中第一项是角函数，第二项是径向函数。然后，根据基于对称函数参数的选择的角函数和径向函数的具体数目，进一步扩展该数。

另一个挑战是原子几何环境的高维。例如，在环境条件下，一个半径为6 Å的液态水的典型截止球包含大约90个原子(270维原子环境)，而当Rc = 10  Å时，原子数量增加到420个(1260维原子环境)。因此，在二元系统中，每个原子环境中使用40到100个对称函数的典型hdnnp已经有效地执行了某种形式的降维。



为了克服对称函数数量的组合增长，有人建议将特定元素的信息直接包含到acsf中。一个例子是加权原子中心对称函数(wASCF)，它放弃了对单个元素组合使用单独的函数，而是在一组径向和角函数中使用与元素相关的加权函数，从而得到

![image-20220210105928310](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210105928310.png)

其中Zj和Zk是核电荷，而且已经证明，即使是简单的选择，如g(Zj) = Zj和h(Zj, Zk)，也能得到满意的结果。



##### Forces and Stress

能量表达式可以计算解析导数，这是获得分子动力学模拟或几何优化等应用所需的原子力所必需的

![image-20220210110038167](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210110038167.png)

从这个方程中可以看出，链式法则必须应用于考虑从笛卡尔坐标到原子中心对称函数的转换，从而产生两个偏导数乘积的和。Nsymi是用来描述元素i的原子的局部化学环境的对称函数的数目。原子NNs的结构给出了原子能量相对于对称函数的一阶偏导数，而对称函数对笛卡尔坐标的偏导数则由acsf的函数形式定义(见第3.3节)。以类似的方式，其他与梯度相关的性质，如应力张量和Hessian，也可以用解析形式得到。

![image-20220210110124783](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210110124783.png)

说明力量对环境的依赖性。中心原子的力分量用黑色表示，取决于半径为Rc的截止球内所有橙色原子的能量，用红色表示。因为这些原子能量取决于自己的原子内的原子位置的环境(一个例子是显示为黑色虚线圆)总共橙色区域内所有原子的位置,也就是说,所有橙色和浅蓝色原子,判断中心原子的力量组件。

根据定义，能量只取决于相邻原子的位置直到截止半径，力取决于Rc内所有原子的原子能量Eji。由于反过来这些原子能量依赖于它们各自的完整的局部环境，力可以有效地依赖于原子的位置，最高可达两倍的截止半径周围的兴趣原子。应该注意的是，在HDNNP中的原子力不是作为独立的附加输出神经元获得的，而是作为能量的解析导数，它们依赖于与原子能贡献相同的重量参数。



![image-20220210110301144](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210110301144.png)

动力学应力的贡献kinσ__是一种动力学性质，可由MD模拟中原子速度vi和原子质量mi计算得到。它的分量由

![image-20220210110318843](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210110318843.png)

其中α和β为{x,y,z}， vkα和vkβ为速度矢量的分量。V为模拟单元的体积

静态应力静态σ__取决于原子的位置，可以从HDNNP力计算:

![image-20220210110353735](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210110353735.png)

其中，Fjβ是j原子在β方向上的力分量

通过对所有原子的贡献求和，就得到了完整的静态应力张量元:

![image-20220210110437408](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220210110437408.png)



#### THIRD-GENERATION NEURAL NETWORK POTENTIALS

##### 结构与能量表达

自经典力场出现以来，长程静电相互作用一直是原子势的中心组成部分。在大多数情况下，简单力场依赖于固定的部分原子电荷，这取决于对原子类型的分类

在ML势中包含长程静电相互作用，不仅可以考虑到超出截止半径的相互作用，而且可以以库仑定律的形式包含有物理意义的能量项，它不代表一个近似，因此与介绍中给出的ML势的定义兼容。更好地描述PES的远程部分也可能使截止半径减小到更小的值，这将有助于在减少的构型空间中采样，并允许集中使用灵活的短程原子能贡献来描述共价相互作用，没有简单的解析泛函关系。这些考虑导致了第三代HDNNPs的发展，它通过引入环境依赖的原子电荷来定义

**![image-20220211105039853](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220211105039853.png)**

![image-20220211105133598](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220211105133598.png)

第三代HDNNPs的培训程序基于以下步骤，以避免重复计算静电和近程部分的静电能量贡献:

1. 原子电荷网络使用电子结构计算得到的参考原子电荷进行训练
2. 通过去除由电荷网络给出的电荷计算出的静电能，可以从参考总势能中提取出短期能量。
3. 利用建立的原子环境与部分电荷之间的函数关系，也可以计算出静电力，并将静电力从参考力中移开，得到力的近程部分
4. 提取的近程能量和力贡献用于训练近程原子神经网络，方法与第二代核神经网络相同
5. 在实际应用中，分别计算近程和静电力和力，并将它们结合起来，得到完整的总能面。通过构造，避免了对静电相互作用的重复计算，因为短距离和电荷网络被训练来提供正确的总目标能量和力



##### 静电力

对于非周期系统和第三代HDNNPs中环境依赖电荷的情况，必须根据NN电荷和库仑定律以一致的方式确定力：

**![image-20220211105532084](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220211105532084.png)**

其中，Rij  = |Ri−Rj|， Qi和Qj为NN部分原子电荷。前两项可以合并，首先拆分这三个和，并在第二个和中重命名索引，以产生：

![image-20220211105555102](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220211105555102.png)

进一步考虑了由笛卡尔坐标变换到对称函数的链式法则：

![image-20220211110016287](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220211110016287.png)

力的近程部分可以从参考电子结构计算中提取为：

![image-20220211110031048](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220211110031048.png)

由于排斥部分的能量急剧增加，在DFT计算中包含色散相互作用的经验修正方案中也遇到过类似的问题，并得到了解决。为了克服这个问题，对于第三代HDNNPs，建议采用下列筛选功能：

![image-20220211110105875](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220211110105875.png)

在屏蔽半径Rscr以下的原子间距离较短时，静电能被屏蔽到零

尽管有这些进展，第三代HDNNPs并不经常使用，除了针对广泛的有机分子的高可转移性的潜力。主要原因是，对于许多系统，特别是凝聚系统，超出截止线的长程静电能贡献被有效屏蔽，因此非常小，这是第二代HDNNPs中的近程原子能无法描述的。将能量误差降低几个meV/原子的可能性，必须与训练第二组原子核网络有关，这也使评估势的计算成本增加了一倍，除了相对要求较高的埃瓦尔德和之外，这可以成为大周期系统的MD仿真的速度决定步骤。



#### FOURTH-GENERATION NEURAL NETWORK POTENTIALS

##### 非定域性的挑战

第三代HDNNPs的主要改进是包含了长程相互作用，在大多数情况下是静电，但也考虑了弥散相互作用，可以在很大的距离上起作用

虽然这是一个明显的进步相比,许多经典力场和传统经验势使用元素或atom-type特定的固定支出,依赖环境的使用费用是不够的,如果长期存在电荷转移。在这种情况下，一个原子的电荷可能强烈地依赖于结构或电子在系统中非常远的地方发生的变化，而不受其局部化学环境的影响。此外，给定系统的不同电离态也会改变整个系统的电荷分布，仅依靠局部结构信息的方法无法区分这些电离态。在化学、分子生物学和材料科学中，从分子到复杂的凝聚态系统，对于许多类型的化合物和反应，远程电荷转移是无处不在的。

![image-20220211113014487](C:\Users\19684\AppData\Roaming\Typora\typora-user-images\image-20220211113014487.png)

以周期性极性氧化锌表面为例进行转移。在这里，最顶层锌原子的局部环境不包括板的底部，因此，氧封端（图13a）和氢封端（图13b）ZnO[0001]表面的局部Zn几何结构是相同的，而整体偶极矩、整体电子结构、，电荷分布也不同。因此，在这两种情况下，在第三代HDNNP中，顶层锌原子被赋予相同的原子能贡献和电荷，这在质量上是不正确的

![image-20220211113159833](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220211113159833.png)

并且事实上，即使是最基本的力场也包括基于固定电荷的远程相互作用，则可以使用依赖于环境的电荷来包含远程相互作用。另一方面，非局域相互作用依赖于全局电荷（再）分布，因此不能用第三代HDNNP来描述。可以处理长程电荷转移的势，即非局域电荷依赖性，定义了第四代HDNNP。电子结构的这些远程甚至全局变化可能是由遥远的官能团、质子化和脱质子化等反应、固体中的掺杂或系统总电荷的变化引起的。

##### The CENT Approach



![image-20220212153808396](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212153808396.png)

电荷平衡神经网络技术（CENT）的结构，用于含有元素a的Na原子和元素b的Nb原子的二元系统。首先，将笛卡尔坐标转换为描述原子环境的原子中心对称函数向量。它们作为原子神经网络的输入，产生与环境相关的电负性χ，然后用于电荷平衡方案，以确定原子电荷Qiμ。使用公式38计算这些电荷的总能量。 

该方法允许电荷密度在整个系统上重新分布，以最小化静电能量。起点是使用原子电荷Qi的二阶泰勒展开式作为近似能量表达式 

![image-20220212153855874](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212153855874.png)

Ei0是自由中性原子的能量，χi是原子电负性，Ji是通过拟合获得的元素特定硬度项。电荷密度ρ=∑iρi由球形高斯电荷给出 ：

![image-20220212154107590](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212154107590.png)

宽度由αi定义。利用这些高斯电荷分布，能量可以改写为 ：

![image-20220212154126834](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212154126834.png)

然后，假设电负性χito与环境有关，在电荷平衡过程中确定系统中的电荷分布。如图15所示，它们采用原子神经网络构造为局部属性。与前几代HDNNP一样，这些原子NN的输入向量是ACSF。元素特定的原子神经网络的权重参数被优化，以产生电荷，使等式38中的能量最小化，也就是说，电荷仅通过电负性间接确定，它们不再是局部性质。



![image-20220212154225386](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212154225386.png)

其中矩阵元素Aij由下式给出：

![image-20220212154239909](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212154239909.png)

这些方程的唯一解总是可以找到的，并且Qtot不需要为零，因此该方法也适用于带电系统。在最初的应用中，人们发现eq  38的分能量表达式提供了非常高质量的结果，尤其是对于以离子键为主的系统。尽管如此，如果这些方法适用于这些系统，那么与第二代和第三代HDNNP相比，这些系统的总能量精度大约低1个数量级。CENT方法的主要成就是首次在ML势的背景下以定性正确的方式包含长程电荷转移。此外，它代表了第一种同时适用于不同全局电荷系统的最大似然电势。 



##### Becke Population Neural Networks



Xie、Persson和Small已经朝着包含远程电荷转移的方向迈出了重要的一步。197他们介绍了一种基于神经网络的方法，称为贝克总体神经网络（BpopNN）（见图16），适用于具有不同全局电荷的系统。利用从电子密度导出的Becke原子布居数pi，能量E[p]被表示为基于原子的布居向量p={pi}的函数，通过在维持系统给定总电荷的约束下改变布居数，该能量函数在称为“SCF-q”的自洽过程中最小化。使用约束DFT以受控方式获得不同总体向量的训练数据，这些数据表示要映射的额外自由度。

![image-20220212161002930](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212161002930.png)

Becke群体神经网络(BpopNN)的结构示意图能量(eq  42)取决于原子位置、核电荷和原子居群，它们被转换为SOAP描述符，作为原子神经网络的输入向量。使系统总能量最小的种群是自洽确定的。

BpopNN总能量被写成原子能贡献Ei的总和，由13.2Å的大截止值定义，并由元素特定神经网络表示为：

![image-20220212160443167](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212160443167.png)

有一个额外的原子电荷依赖的原子内能量 :

![image-20220212160458501](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212160458501.png)

它具有与元素相关的函数形式，并被用作适用于具有不同全局电荷的状态的原子能量基线。此外，两两长程静电相互作用由:

![image-20220212160524387](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212160524387.png)

在这里，κ是与原子神经网络参数一起优化的元素特异性参数。所使用的描述符是SOAP描述符的修改版本，包含核电荷、占据量和各自中心原子的原子位置，此外还依赖于截止球内相邻原子的信息。



##### 4G-HDNNPs

最近，第二代HDNNPs和CENT方法的优势被结合在第四代高维神经网络潜力(4G-HDNNP)的开发中，包括远程电荷转移以及对局部键合的精确描述。和第三代HDNNPs一样，4G-HDNNPs的总能量是近程部分和远程静电能量的总和

![image-20220212161105643](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212161105643.png)

![image-20220212161114221](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212161114221.png)

与CENT一样，电荷平衡依赖于环境相关的电负性，而电负性由单个原子神经网络表示，对于给定的元素，它们被限制为相同的。与CENT的程序不同，在这些网络的训练过程中，目标并不是最小化静电能量。相反，电负性被构造来重现由DFT计算得到的Hirshfeld电荷的参考数据集。由于这些电荷不是直接训练的，而是间接通过电负性，它们取决于整个系统。然后利用这些电荷来计算长程静电能Eelec。

为了避免重复计算能量项，将静电力能和力从各自的参考数据中去除，剩下的贡献用短程网络表示为原子能量的总和

![image-20220212161208103](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220212161208103.png)

与第二代和第三代HDNNPs以及CENT方法相比，4G-HDNNPs具有更广泛的适用性，因为它们适用于具有和的系统

没有非局部电荷转移，能够高精度地描述共价键，与CENT相比，它不局限于具有主要离子相互作用的材料。与CENT和BpopNNs一样，4G-HDNNPs也可以被构造成同时描述一个系统的不同全局电荷状态，例如，一组电离簇或(de)质子化反应。在最初的应用中，4G-HDNNPs为有机分子、离子和金属团簇以及在周期性氧化物表面吸附的金属团簇提供了非常有前途的结果还有待观察的是，4GHDNNPs在多大程度上能够描述主要的非静电长程相互作用，例如，由缺陷引起的半导体费米能级位移引起的相互作用。第一个迹象表明，4G-HDNNPs对远处的掺杂敏感，这已经被证明是在表面上的小金属簇的掺杂依赖的吸附几何形状，但还需要进一步的分析，以从提供给短程神经网络的非局部信息中解出静电效应。



#### 高维神经网络电位的构建

HDNNPs的开发是一个多步骤的过程，包括基础参考数据集的构建，这是在电子结构计算中获得的，原子神经网络的权重参数的确定，以及电位的最终验证。所有的步骤都是同等重要的，彼此不能分离，因为它们在一个自一致的工作流中是紧密相连的。



##### 参考数据集建构

参考数据集中结构的选择是构建任何ML势的重要步骤，因为潜在的函数形式不是基于物理原理。相反，PES的物理形状必须从参考数据中提供的资料中了解。虽然在原则上，大多数低维系统将允许基于系统构型网格的第一代NNPs的开发，但即使是早期的NNPs也通常依赖于数据点的非均匀分布，这使人们能够关注与化学最相关的构型。



可以通过多种方式获得初始数据集。如果主要的结构特征，如晶体结构或分子实体已知，则可以基于这些信息构建第一个数据集，理想情况下还包括扭曲的结构和热波动。对于分子系统，也可以使用正态采样。通常，从头开始的分子动力学模拟被用来对构型空间的某些部分取样作为起始点。

通过分子动力学，然后可以通过电子结构方法计算。当力场预测结果与电子结构结果偏差过大时，可以利用数据点来提高电势。然而，可能的改进受到通常非常不灵活的功能形式的限制，这种方法的一个缺点是需要苛刻的电子结构计算，甚至只是为了评估潜力的质量而需要计算。

在这方面，像HDNNPs这样的ML潜力提供了一个实质性的优势，因为它们具有高度的灵活性，这是由层次函数形式中的大量参数产生的。由于它们是非物理的，即无偏倚的，功能性的形式和高度灵活的结构，在训练过程中可以获得许多不同的局部极小值。所有这些局部极小值将充分重现已知的训练集，但对于远离训练数据的结构的预测，在可用的hdnnp中往往会有很大的不同，对于这种识别，只需要对几种不同的HDNNPs进行廉价的评估，而不需要执行昂贵的电子结构计算。这个被称为“主动学习”

主动学习构造参考数据集的一般过程如图18所示。目标是用黑色表示未知能量曲线。

![image-20220213192521367](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220213192521367.png)

现在我们训练了几个红色、绿色和蓝色的HDNNPs，它们很好地重现了这些训练数据附近的区域。在训练点之间的灰色区域的大间隙中，所有的势能都预示着非常不同的能量和力量。这些差异可以用来为电子结构计算选择额外的参考结构，这些参考结构已被添加到图b中，以减少没有很好采样的区域。这一过程不断重复，直到面板c覆盖了所有相关的配置空间。只要HDNNPs预测相同的PES在一个预定义的错误阈值(通常是几个meV/atom)内，应用的潜力就已经准备好了。

通常，用于构建hdnnp的数据集非常大，包含数以万计的电子结构计算。因此，有效的电子结构方法，如DFT是最常用于凝聚态系统。近年来已经制定了若干战略来减少这种努力。除了上面描述的主动学习方法(它允许有选择地对那些重要但缺失的结构进行电子结构计算)之外，已经发展出了一些方法来只选择那些在结构上不同的构型此外，通常的做法是将不同类型的系统，如分子、团簇、体积和表面计算组合在一个单一的训练集中。

在这些情况下，在电子结构计算中使用非常高水平的收敛是非常重要的，特别是关于k点网格的周期性计算，以保持数值噪声最小。即使对同一方法和物理模型使用不同的电子结构代码，通常也会导致数值不一致的数据，不能很容易地组合起来进行训练。此外，已经证明用于大系统的HDNNPs可以由较小的分子片段构建，

为了从电子结构计算中获取最大数量的信息，除了总能量外，现在还经常使用力来确定神经网络参数，甚至应力张量也被建议用于训练hdnnp 力提供了实质性的优势，特别是对于较大的系统，因为总能量可以遭受误差补偿之间的个别原子能贡献。

#####  Training

对于给定的数据集，必须确定权重参数的值。这通常是通过重复循环数据来完成的，在大多数情况下包括能量和力。

数据集的完整表示称为历元。所需的梯度是能量和力误差的导数，即相对于神经网络权值，神经网络预测和电子结构计算的参考值之间的差异要最小化。由于电子结构数据不依赖于神经网络权值，因此只需要神经网络能量和力相对于神经网络权值的梯度。使用**反向传播**可以非常有效地计算出这些梯度。

在最基本的情况下，要最小化的错误函数Γ采用简单的形式：

![image-20220214101619135](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220214101619135.png)

不需要将参考能量预先划分为单独的原子能量。相反，在训练过程中，总能量被HDNNP自动分配成单独的原子能量。这些原子能量不是物理上的可观测的，而只是用来表示总能量的数学辅助量。

现代的HDNNPs很少通过最小化简单的损失函数(如eq  47)来训练。相反，经常使用复杂的算法，如卡尔曼滤波器。事实上，自适应、全局、扩展的卡尔曼滤波器很早就被提出用于训练NNPs.卡尔曼滤波的一个重要特性是频繁的重量更新后的每一个信息,即能量或动力组件,而其他先进的优化算法只依赖一个重量更新每个时代,也就是说,处理后完整的数据集,使用一个平均梯度。因此，后一种算法更适合并行化，而每条信息的显式使用与随后的权值更新，使卡尔曼滤波器在达到局部最小值所需的迭代方面特别有效。

HDNNP的优化从一组初始权值开始，这些初始权值可以随机选择，也有多种初始化方案。

除了初始权值外，还必须选择原子网络的结构。对于大多数系统，50到100个acsf和2到3个隐藏层，每个包含15到45个神经元，就足以构建非常精确的电位，如果添加或删除少量神经元，误差变化很小。

ML电位的误差通常由均方根误差(RMSE)测量：

![image-20220214101957759](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220214101957759.png)

有时也可以用平均绝对偏差(MAD)来代替。一般的RMSE能源必须谨慎地对待作为测量的准确度误差补偿可能存在的原子的能量,原子能量的特定的值有一个高度的不确定性,如果他们被训练的总能量为目标属性。因此，对于大型系统，在训练过程中监测部队的均方根误差是很重要的

由于HDNNPs中权重参数Nw较多，对于具有Nhidden层的原子神经网络，本文给出了：

![image-20220214102200238](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220214102200238.png)

Nk是相应层中的神经元数量，通常需要优化数千个参数。因此，权值的确定是一个非常高维优化问题，没有希望找到全局最小值。然而，通常需要找到适合可靠模拟的足够精确的局部极小值，然后在应用之前需要仔细验证。



##### Validation

像所有的ML势一样，HDNNPs只从包括在训练集中的信息中学习原子的相互作用。因此，失败和不准确性可能主要来自三个来源:**可用数据**、**模型中的参数数量**(即定义灵活性的原子网络的架构)以及**优化过程的限制**(例如，陷入局部最小值)。

HDNNPs的验证是一个多步骤的过程：

1. 基于初始数据集构建一个初步的HDNNP，该数据集可能包括从头算MD轨迹或甚至随机结构。此数据集不包括所有相关的配置空间。要做的第一个分析是确定坐标的范围，即描述符，已经被数据集覆盖的值。这一点很重要，因为神经网络在用于训练集之外的外推时表现得很差，如图19中描述函数G1所示的一维系统示意图所示。![image-20220214102448379](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220214102448379.png)                                                                    在区间[G1,min,  G1,max]之外的能量具有非常高的不确定性，预测的能量和力可能是非常错误的。幸运的是，即使在高维系统中，通过将每个感兴趣的原子环境的ACSF值与训练集覆盖的相应区间进行比较，也可以很容易地自动检测这些外推情况。如果发现一个或多个对称函数的外推情况，则可以对该结构进行电子结构计算，然后将其纳入训练集，以扩展HDNNP的适用性。
2. 下一步，使用早期停止方法(图20)检查参考数据集是否采样密集，或者在某些区域可用结构是否过于稀疏.为此，将参考数据集随机分割成一个训练集，用于优化的权重参数, 以及在拟合过程中不使用的独立测试集。然后在拟合期间监视两个数据集的RMSE值。最初，随着HDNNP学习PES的整体拓扑结构，两个RMSE值都降低。经过几次迭代后，测试集RMSE可能会出现一个局部最小值，然后略有增加，而训练集的RMSE进一步减小。这就是过拟合的开始，它是训练数据的一个非常准确的表示，但它会显示出非常大的误差。因此，在基于测试集做出决策之后，通常的做法是使用第三个验证集来评估整体性能。![image-20220214102807037](C:\Users\19684\AppData\Roaming\Typora\typora-user-images\image-20220214102807037.png)                                                                          早期停止法是训练神经网络的标准程序。然而，这个测试不足以确保参考数据集涵盖所有相关配置。原因是，在早期停止方法中，只有那些点可以分配到一个训练集和一个测试集，原则上在参考集中是可用的。但是，如果完全缺少某些类型的配置，则这些结构的不可靠描述将无法检测到。蓝色参考数据点很好地覆盖了浅蓝色区域，而红色的数据点离这些数据点很远。如果我们对红点的能量和力的预测感兴趣，无论是外推检查还是早期停止方法都不会检测到这些点的可靠性可能存在的问题。这个例子清楚地表明数据集的验证、训练和构造是密切相关的，不能单独处理。相反，包含这三个步骤的整个过程必须迭代地重复，如图22所示，直到得到一个聚合的数据集和势。![image-20220214103016543](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220214103016543.png)

  图23中的流程图总结了整个验证过程:

![image-20220214103058326](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220214103058326.png)

一旦一个电位通过了所有的步骤，包括外推检查，早期停止，以及对洞的主动学习搜索，模拟应该最终进行几个相同质量的hdnnp，不同的，例如，在原子神经网络的体系结构或用于生成训练的随机初始权值的初始种子中，以确保结果独立于特定的拟合和物理意义。

如果数据噪声过大，例如在电子结构计算中只选择了适度的收敛级别，或者甚至包含了错误的数据点，能量和力的误差就会相当大。为了识别错误的电子结构计算，有一个相当简单但有效的测试(图24)。在这个测试中，所有数据的错误点与另一个HDNNP的各自误差绘制![image-20220214103201976](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220214103201976.png)

对于“正态”拟合误差，将在总体误差较低的坐标轴附近发现各自的点，因为很可能大多数点将至少由一个势函数准确表示。因此，在两个hdnnp中，绝大多数的点将在左下角区域找到对应的良好表示。但是，如果点位于右上角，这表明这些点不能被任何hdnnp准确表示。在这种情况下，原因通常是数据集中的信息相互矛盾，这可能会导致，例如，电子结构计算中的收敛问题或由于k点采样不佳而产生的大数值噪声。然后应该调查这些点，并将其从数据集中删除。

另一条注释介绍了学习曲线233的使用，如图25所示。学习曲线经常被用来检查与训练过程中可用的数据点数量有关的势的收敛性。在这方面，它们提供了与早期停止方法相似的信息。最初，第一个HDNNP仅使用可用数据的一小部分来构建。由于少数几个数据点是直接学习的，因此得到的训练集RMSE非常小。

![image-20220214103302595](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220214103302595.png)

由于过拟合，测试集的RMSE非常大。如果现在逐步使用更多的数据，对HDNNP来说拟合问题变得更有挑战性，训练集误差增加，而测试集误差减小。最后，两个误差收敛，表明使用了足够数量的数据点。这种方法很有指导意义，但不幸的是，它与早期停止方法有相同的局限性，即在配置空间中没有可用数据的区域存在不准确性，仍然无法检测到。因此，学习曲线必须小心处理，除非能保证构型空间的完全覆盖。



#### DISCUSSION

##### 适用性和应用程序

在以上讨论的所有代神经网络电位中，到目前为止，第二代HDNNPs类似于等效的第二代ML电位类型，例如GAPs，在整个文献的应用中使用得最频繁。此外，在某种程度上，第一代NNPs仍然在使用，如果低维系统的描述对科学问题是充分的。例如，在气相小分子反应动力学领域，这也可以用第一代的NNPs进行全维描述。虽然第二代HDNNPs也同样适用于这些系统,使用第一代的优势NNPs采用单一神经网络表达全球能源与约等于精度较高的计算效率,作为第二代HDNNPs涉及的评估每个原子一个神经网络。对于小系统，通常不需要明确处理长程静电和色散相互作用，因为第一代NNPs中的全局描述允许充分考虑这些相互作用，而不考虑原子距离。然而，大多数感兴趣的系统包含大量的原子，因此超出了第一代NNPs的范围，而第一代NNPs适用于最多只有10个原子的系统。

与电子结构计算相比，第二代HDNNPs允许原子性的扩展模拟有两种方式。首先，与训练集结构中的原子数量相比，系统的大小可以大大增加。当训练数据的系统规模限制原子的数量,可以参考电子结构的计算处理,也就是说,几百个原子的DFT,一旦HDNNP训练更大的系统可以通过增加研究原子的数量相应的神经网络在图5。对于这种系统大小的扩展，必须考虑到，在扩展系统的原子环境中出现的原子配置也必须出现在训练集的较小结构中，以避免不可靠的推断。

另一方面，如果在大规模模拟中发现有问题的环境，则可以利用缩小尺寸的系统进行电子结构计算，将其提取并纳入训练集。由于每个原子有一个神经网络，可以独立计算，所以该方法非常适合并行化。

目前的CPU架构允许计算大约每秒500个原子和CPU核心的能量和力。除了系统大小的增加，hdnnp支持的第二个可能的原子模拟扩展涉及可以计算的配置数量。分子动力学模拟可以在更长的时间尺度上运行，比如包含数千个原子的系统可以运行数十或数百纳秒，

此外，其他模拟技术，如蒙特卡罗模拟，受益于大量的配置，可以处理虽然原子模拟的时间和长度尺度与电子结构方法的扩展是HDNNPs的一个明显优势，这种扩展也可以使用传统的经验势和力场，它们通常可以用较少的工作量来构建参考计算，甚至使用成本更低。这就提出了一个问题，hdnnp系统在哪些方面提供了额外的优势，使其成为比传统类型更好的选择。HDNNPs和ML电位的一个重要优势是，**不需要了解原子相互作用的物理功能形式**，就可以达到非常高的精度。这使得构建难以描述的系统的势能成为可能，例如与非常规原子环境的界面、非晶系统、238和有机-无机杂化系统，以及在不同部分存在非常不同类型的键合的系统。例如分子中的共价键和表面上的金属键，都可以被研究。此外，HDNNPs具有**活性**，能够描述键的形成和断裂，这对于研究化学过程显然是必要的.

HDNNPs，特别是第二代，已经应用了近15年的多种系统和广泛的问题，在化学、物理和材料科学。

![image-20220214103720170](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220214103720170.png)

HDNNPs最早的应用是针对单质块材料，如硅，这是开发和评估原子间势的一个重要的基准系统，对其进行了压力诱导相变的研究可以证明，HDNNPs能够用DFT精度描述包含非常不同原子环境的广泛结晶相的结构和能量特性元素固体的进一步应用研究了碳中石墨到金刚石的转变和钠的熔化特性。此外，还研究了大量的二元固体，如ZnO.

除了大块固体外，表面也受到了很大的注意HDNNPs已被用于研究吸附质结构，包括有效鞍点搜索和表面吸附诱导的facet,负载的纳米颗粒和团簇的形态，和分子表面散射过程。

此外，HDNNPs还成功地构建了一系列非周期体系，如有机分子，和多通道体系的气相反应，甚至高度可转移的有机分子通用电位，

水，最重要的溶剂，也受到了很多关注，以小簇的形式，但最重要的是体积。液态水无疑是最常被研究的系统之一，尽管所有的努力，许多有趣的性质对计算机模拟仍然是一个挑战，要么因为模拟对从头开始MD太苛刻，要么因为经典水势的准确性有限。HDNNPs研究了分散相互作用和氢键网络对熔化温度和密度异常的作用,六方冰和立方冰的相对稳定性，和核量子效应的作用利用HDNNPs研究水分子的解离和重组的可能性在模拟氢氧化钠水溶液中也是必不可少的。

此外，金属和氧化物表面的界面水也受到了广泛的关注。

这些应用大多是指第二代HDNNPs，而第三代HDNNPs很少使用，因为包含了环境相关的费用，带来了大量的计算开销，对许多系统的潜力只有很少的整体改善。HDNNPs的第四代，提供了更多对长期相互作用以及包括非局部效应的准确描述才刚刚出现。

##### 展望

HDNNPs的开发就像ML潜力的开发一般是一个非常活跃和快速增长的领域，许多挑战仍有待解决。除了第四代HDNNPs的进一步发展，数据集的构建，将额外的物理元素纳入能量表达式，新的描述符类型的发展，多元素系统的处理，以及额外物理性质的预测只是几个例子。

在HDNNPs的发展中，可能最苛刻的步骤是通过电子结构计算来构建大型参考数据集。这些计算需要进行高水平的收敛,嘈杂的数据有问题,有两个原因:第一,矛盾的数据会存在类似的原子环境与不同的能量和力量,从而防止顺利融合权重参数的优化。第二,在嘈杂的数据的情况下,一些HDNNP之间的偏差和引用数据tolarable  PES不应该复制数值噪声,但作为一个正式的拟合结果错误仍然相对较高,因此是毫无用处的作为潜在的准确性的措施。

HDNNPs的一个优点是，通过构建，它们允许以一致的方式将具有不同原子数的参考结构具有或不具有周期性边界条件组合在一个单一的训练集中，这使得以最有效的方式构建参考数据集成为可能。此外，没有被很好地描述的原子环境，可以使用较小的分子片段来描述所需的环境，从而避免了系统其他部分中冗余信息的计算，

最近，人们提出了一种方法，通过将部分PES表示为二体项和三体项的和，将ML势的能力集中在最难以描述的多体项上。这是一个非常有希望的步骤，特别是如果结合显式静电处理，如在第三代和第四代HDNNPs。改善静电的另一种途径是超越点电荷模型，在极端情况下引入高阶多极，通过ML模型表示电子密度。高阶多极的加入也可能进一步提高第四代潜力。

另一种可能的改进，目前正受到很大的注意，是考虑分散相互作用，在大多数情况下不像静电那样有问题，但对某些系统可能积累起来，形成一个不可忽视的重要贡献正式地说，这种弥散校正的HDNNPs虽然没有那么长远，但可以被归类为第三代。包含色散相互作用而不进行显著的截断的一个直接解决方案是直接结合最初为DFT开发的色散校正方案，

针对包含多个元素的系统，提出了一系列建议，以促进hdnnp的建设。一个可能的起点是，通过将关于原子核电荷或离子电荷的信息直接合并到函数形式中，从而减少描述符的数量。另外，也减少了截断Onat等人在植入神经网络和分层神经网络中采用了不同的方法在这种方法中，首先构造所涉及元素的hdnnp，然后将其与描述其相互作用的其他nn结合起来。另一种方法是SingleNN，它是一种修改后的HDNNP，在元素之间共享权值迄今为止，有一个领域在很大程度上仍未开发，即ML电位的性质和可解释性分析。对于经验电位，已经证明与DFT.相比，可以出现大量的伪极小值，因为ML电位具有很高的灵活性，也可以预期会出现同样的问题，但迄今为止还没有报道过。关于ML势的解释，对于其他类型的势，人们试图从物理上解释中使用的单个原子能量，但HDNNPs的第一个结果表明，原子能量的数值只是数学上的辅助量，即使是同样质量的电势也会有很大的不同。

最近在描述符和进一步物理性质的预测领域也取得了进展，如用HDNNPs表达原子自旋和氧化态，以及张量性质的描述，这一领域目前正在迅速发展其次是基于描述符的基于深度张量神经网络处理笛卡尔坐标和核荷的信息传递方法，学习了这些特征，代表了一个重要的新发展方向。由于有效交互作用范围取决于传递的步数，且涵盖了第二代和第四代势的全部范围，因此很难按照本文所述的代层次进行分类。在这一点上，对第五代HDNNPs的可能扩展仍处于推测状态，但这种可能性可能涉及到将ML更紧密地整合到电子结构方法中。虽然在本质上不同于原子机器学习势，但已经证明，例如，交换相关泛函可以基于ML321改进，

最后，各种各样的可用软件也取得了很大的进步。继RuNNer代码之后，是第一个实现HDNNPs的代码，并已被用于其开发，现在有各种各样的替代实现，提供了一个很容易进入该领域的入口。像tensorflow这样的现代软件库极大地促进了其中一些代码的开发



#### SUMMARY

本文综述了高维神经网络电位的研究现状。为了达到这个目的，引入了一个由四代人组成的分类方案，它也可以以类似的方式应用于许多其他类型的机器学习潜力。

第一代NNPs适用于低维系统，如真空中的小分子或与冻结表面相互作用的小分子。他们利用势能的全局描述，通常只使用一个前馈神经网络。

第二代NNPs通过将总能量表示为环境相关的原子能量的总和(由单个原子神经网络计算)，允许在全维中使用任意数量的原子对高维系统进行寻址。第一次,也是迄今为止最常用的第二代HDNNP  Behler提出的方法和Parrinello原子的输入神经网络是由一组atom-centered对称函数描述当地的原子的化学环境截止半径。

长距离相互作用，最显著的静电，在第三代HDNNPs中没有截断，它增加了第二组提供环境依赖电荷的原子神经网络。与第二代HDNNPs相对应的原子神经网络，然后用于构建剩余的短程部分能量。第三代HDNNPs是适用的，如果电荷只取决于当地的化学环境。

在远距离电荷转移或多重电荷状态的情况下，必须使用第四代HDNNPs，它允许在整个系统中电荷的全局重新分配。第四代HDNNPs目前刚刚出现，但已经提出了几个非常有前途的方法，以缩小目前最先进的ML潜力的适用性方面的剩余差距。

如今，特别是，第二代HDNNPs已经发现在原子模拟中使用，因为对于许多系统，它们提供了非常高的精度，接近底层的电子结构计算，而它们的效率允许计算数以百万计的配置与数万个原子。
