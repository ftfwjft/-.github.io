# Scientific Reports|通过深度强化学习优化分子



今天给大家介绍Zhenpeng Zhou , Steven Kearnes等人在Nature/Scientific Reports上发表的文章“Optimization of Molecules via Deep Reinforcement Learning”。这篇文章主要是提出了一个Molecule Deep Q-Networks (MolDQN)框架，通过结合化学领域知识和先进的强化学习技术来进行分子优化。作者采用直接对分子修改的方式，来保证100%的化学有效性；而且在任何数据集上都不进行预训练，以避免可能的偏差；最后通过与其他几种最近发表的分子优化算法对比，得出基于MolDQN框架的分子优化可以获得更好的性能。



![img](https://pic1.zhimg.com/80/v2-df2b48cfbb21cc33b8cf01f24d9c5074_720w.jpg)

## 研究背景

化学的一个基本目标是设计具有特定性能的新分子，这在材料设计或药物筛选中尤为重要。 目前，这一过程在时间和成本上代价很大，因为它可能需要几年时间以及花费数百万美元来开发一种新药。这篇文章研究的目的是通过强化学习部分自动化这一过程。

目前的做法都是使用策略梯度方法，而本文是基于价值函数学习。虽然策略梯度方法适用于更广泛的问题，但在估计梯度时受限于高方差；相比之下，通过价值函数学习可以使样本更有效、更稳定。

在这里文中介绍了一种结合化学领域知识和强化学习的分子优化设计，称之为Molecule Deep Q-Networks。通过制定修改分子作为马尔可夫决策过程(MDP)，只允许化学有效的动作来确保生成的所有分子都是有效的；然后，采用基于Molecule Deep Q-Networks框架的深度强化学习技术以及使用特定的性能作为奖励来解决MDP。模型不是在数据集上进行预训练，而是从头开始学习，这样可以扩大搜索空间，避免错过不在数据集中的分子，使之产生更好的性能，即发现具有更好的性质的分子。

## 模型与方法

**2.1 分子修改作为马尔科夫决策过程**

由于生成的分子仅依赖于被改变的分子，因此，分子优化过程可以表述为马尔科夫决策过程(MDP)。

分子的修改或优化可以逐步进行，其中每一步属于以下三类之一：

1. 添加原子，(2)添加键，(3)去除键。



![img](https://pic2.zhimg.com/80/v2-dffd2bdd2b262c935a615e353fc3b7fd_720w.jpg)图1 分子修改的三个过程

限制条件：

- 对步骤的数量增加明确的限制
- 不允许化学无效的行为(违反价态约束)
- 允许原子/键被移除和添加

设置MDP的四大指标 ![[公式]](https://www.zhihu.com/equation?tex=%28S%2CA%2C%7BP_%7Bsa%7D+%7D%2CR%29) ：S表示状态空间，其中状态S∈(m,t),m表示有效分子，t表示采取的步骤数; A 表示动作空间，其中每个动作a∈A是对特定分子m的有效修改； ![[公式]](https://www.zhihu.com/equation?tex=%7BP_%7Bsa%7D+%7D) 表示状态转移概率，文中将状态转换设置为确定性的；R表示状态S∈(m,t)的奖励函数；为了确保最终的状态得到最大的奖励，将每一步状态下的奖励值折扣值设为γ。

**2.2 强化学习**

通过建立函数Q(s,a)，其数学公式为 ![[公式]](https://www.zhihu.com/equation?tex=Q%5E%CF%80+%28s%2Ca%29%3DQ%5E%CF%80+%28m%2Ct%2Ca%29%3DE_%CF%80+%5B%E2%88%91_%7Bn%3Dt%7D%5ET+r_n+%5D) ，通过这个行为价值函数计算对状态s采取行动a的未来回报，然后由策略 ![[公式]](https://www.zhihu.com/equation?tex=%CF%80) 决定后续行动。因此，我们可以制定最优策略 ![[公式]](https://www.zhihu.com/equation?tex=%CF%80%5E%2A+%28s%29%3Darg%E2%81%A1max_a%E2%80%8AQ%5E%7B%28%CF%80%5E%2A+%29%7D+%28s%2Ca%29) 。

由于有确定性的MDP和精确的环境模型，因此，可以选择近似值函数 ![[公式]](https://www.zhihu.com/equation?tex=V%28s%29%3Dmax_a%E2%80%8AQ%28s%2Ca%29) 的最大值和计算从s到s’的状态奖励作为 ![[公式]](https://www.zhihu.com/equation?tex=Q%28s%2Ca%29%3DR%28s%5E%E2%80%98%29%2BV%28s%5E%E2%80%98%29) 。然后采用deep Q-learning算法对Q函数进行估计，将神经网络函数逼近器称为参数化的 ![[公式]](https://www.zhihu.com/equation?tex=Q%28s%2Ca%3B%CE%B8%29) 函数,通过最小化损失函数 ![[公式]](https://www.zhihu.com/equation?tex=l%28%CE%B8%29%3DE%5Bf_l+%28y_t-Q%28s_t%2Ca_t%3B%CE%B8%29%29%5D) 来训练逼近器，其中 ![[公式]](https://www.zhihu.com/equation?tex=y_t%3Dr_t%2Bmax_a+Q%28s_%7B%28t%2B1%29%7D%2Ca%3B%CE%B8%29) 表示目标值， ![[公式]](https://www.zhihu.com/equation?tex=f_l) 表示损失函数。

![img](https://pic1.zhimg.com/80/v2-15b9e0fff6c70902802fb5d5a8a904d8_720w.jpg)

**2.3 多目标强化学习**

在多目标强化学习条件下，环境将在每个步骤t处返回一个奖励向量，每个目标有一个奖励 ![[公式]](https://www.zhihu.com/equation?tex=i.e.+%28r_t+%29+%E2%83%97%3D%5Br_%7B%281%2Ct%29%7D%2C%E2%80%A6%2Cr_%7B%28k%2Ct%29%7D+%5D%5ET%E2%88%88R%5Ek) ，其中k是目标数。目标可以是建立一组Pareto最优解，也可以是满足决策者偏好的单个或多个解。本文对后一项作了调整，实现了“缩放”奖励框架，以实现多目标优化；然后引入用户定义的权重向量 ![[公式]](https://www.zhihu.com/equation?tex=w%3D%5Bw_1%2Cw_2%E2%80%A6%2Cw_k+%5D%5ET%E2%88%88R%5Ek) ，所以标量奖励可以计算为 ![[公式]](https://www.zhihu.com/equation?tex=r_%7B%28s%2Ct%29%7D%3Dw%5ET+%28r_t+%29+%E2%83%97%3D%E2%88%91_%7B%28i%3D1%29%7D%5Ek%E2%80%8A+w_i+r_%7B%28i%2Ct%29%7D) ；最后，MDP的目标就是最大限度地增加累积的标量奖励。

**2.4 训练期间的探索与利用**

一方面，对所有状态的奖励没有完全的了解，如果不断地选择已知最高奖励的最佳行动，将永远不会了解其他状态是否有更高的奖励；另一方面，如果总是随机选择行动，将不会得到尽可能多的奖励。

所以文中采取ε贪婪的方式，以 ![[公式]](https://www.zhihu.com/equation?tex=1-%CE%B5) 的概率预测最佳动作，以的概率使随机动作均匀化。利用随机值函数进行深度探索，建立独立的Q函数 ![[公式]](https://www.zhihu.com/equation?tex=%7B%7BQ%5E%7B%28i%29%7D%E2%88%A3i%3D1%2C%E2%80%A6%2CH%7D%7D) ，每个样本都被训练在不同子集上，在每一子集，通过 ![[公式]](https://www.zhihu.com/equation?tex=Q%5E%7B%28i%29%7D) 进行决策。

## 实验结果

将MolDQN与以下最先进的模型进行比较

- 功能树自动编码器(JT-VAE)是一种深度生成模型，它将分子映射到高维空间，并在空间中进行优化分子。
- 目标-增强生成对抗性网络(Organ)是一种基于强化学习的分子生成算法，通过SMILES字符串进行输入和输出。
- 图卷积策略网络(GCPN)是另一种基于强化学习的算法，它结合MDP对分子的图表示进行操作。

**3.1 单属性优化**

从图2中可以看出，在反馈标准设为惩罚logP的优化中，生成的分子显然不像药物，这突出了在使用强化学习时使用多个目标进行奖惩的重要性。而且在没有约束的情况下最大化logP的任务不是一个很好的度量来评估模型的性能，惩罚logP值几乎随原子数线性增加，因此，在不保证原子数相同的情况下比较logP是不合理的。

从表1中可以看出与GCPN等三种算法相比，MolDQN在logP优化上表现出更好的性能，在QED优化中也能表现出与三种先进优化算法相近的性能，这些结果可以部分归因于从头开始学习和搜索范围不限定与特定数据集中的分子。



![img](https://pic2.zhimg.com/80/v2-6b3241b94c15fd0fc4da7c36a106783d_720w.jpg)表1 每种方法发现的前三个分子的性质分数

![img](https://pic2.zhimg.com/80/v2-cc840e9339f005682b19f5c4522c8e55_720w.jpg)图2 性质优化任务中的样品分子



**3.2 多目标优化**

方法：将分子的多目标奖励设置为二维向量，然后用权重w表示分数的权重，用权重表示分数的权重,即 ![[公式]](https://www.zhihu.com/equation?tex=R%28s%29%3Dw%C3%97SIM%E2%81%A1%28s%29%2B%281-w%29%C3%97QED%E2%81%A1%28s%29) 。

结果：图3证明了实验可以成功地优化分子，同时保持被优化的分子具有类似于起始分子的性质。随着权重的增加，优化后的分子与起始分子具有较高的相似性，且优化分子的QED值低于起始分子的QED值；图4中随着权重的增加，QED改进的分布向左移动，因为更高的优先级被放置在相似性上；图5直观地检查了优化的分子，在w>=0.4下产生的分子与起始分子相类似，表明当权重足够高时，训练后的模型保留了原始结构。

![img](https://pic4.zhimg.com/80/v2-ab753bfea7f6ef60091bb3d1983bc04f_720w.jpg)表2 约束优化任务中惩罚logP改进的均值和标准差

![img](https://pic1.zhimg.com/80/v2-fd6a0de53406921ea3672ec791a576e8_720w.jpg)图3 在不同目标权重下优化分子的相似性

![img](https://pic4.zhimg.com/80/v2-718c99a5115d3d6c38c69054b53d4daf_720w.jpg)图4 QED改进的经验分布

![img](https://pic2.zhimg.com/80/v2-93458d1de431679ac12d0dc23652f00d_720w.jpg)图5 多目标优化任务中的分子结构图

**3.3 最优性vs多样性**

最优性和多样性之间存在着联系，如果不引入随机性，执行学习的政策将导致产生确定的分子，文中有三种方法可以增加产生的分子的多样性：

- 选择函数 ![[公式]](https://www.zhihu.com/equation?tex=Q%5E%7B%28i%29%7D+%28s%2Ca%29) 统一在1,…,H上的每一集作出决策
- 在每个步骤中随机地绘制一个概率与Q函数成正比的动作
- 在评估过程中，在ε贪婪算法中使用非零ε

**3.4 可视化和解释**

在决策的第一步，Q网络预测每个动作的Q值，可以观察到，添加羟基是非常有利的，而破坏分子的环结构是极其不利的；Q值是未来奖励的度量，因此算法可以选择一个在短期内降低属性值但可以达到更高未来奖励的动作。



![img](https://pic1.zhimg.com/80/v2-2e54f42d070a24f4e0d337df59e5f3c8_720w.jpg)图6 (a)可视化动作的Q值;(b)最大限度地提高QED的步骤



## **总结**

通过将最先进的深度强化学习与化学领域知识相结合，建立了分子优化的MolDQN模型。实验证明，与其他几种已建立的算法相比，MolDQN在生成具有更好的特定性质的分子方面达到了等效或更好的性能。文中还提出了一种可视化决策过程的方法以促进学习优化分子设计的策略。



## **分析源码**

使用了`rdkit`包

[RDKit中文教程 — RDKit 中文教程 2020.09 文档 (chenzhaoqiang.com)](http://rdkit.chenzhaoqiang.com/index.html)

使用`SMILES`描述原子：[SMILES（用ASCII字符串明确描述分子结构的规范）](https://baike.baidu.com/item/SMILES/6655640)

```python
from rdkit.Chem import Draw
from rdkit import Chem
smis=[
    'COC1=C(C=CC(=C1)NS(=O)(=O)C)C2=CN=CN3C2=CC=C3',
    'C1=CC2=C(C(=C1)C3=CN=CN4C3=CC=C4)ON=C2C5=CC=C(C=C5)F',
    'COC(=O)C1=CC2=CC=CN2C=N1',
    'C1=C2C=C(N=CN2C(=C1)Cl)C(=O)O',
]
mols=[]
for smi in smis:
    mol = Chem.MolFromSmiles(smi)
    mols.append(mol)
img=Draw.MolsToGridImage(mols,molsPerRow=4,subImgSize=(200,200),legends=['' for x in mols])
```

输出：

[![_images/NCycle2020-02-10_092304.562537.png](http://rdkit.chenzhaoqiang.com/_images/NCycle2020-02-10_092304.562537.png)](http://rdkit.chenzhaoqiang.com/_images/NCycle2020-02-10_092304.562537.png)

程序定义超参数：

```python
hparams = contrib_training.HParams(
      atom_types=['C', 'O', 'N'],
      max_steps_per_episode=40,
      allow_removal=True,
      allow_no_modification=True,
      allow_bonds_between_rings=False,
      allowed_ring_sizes=[3, 4, 5, 6],
      replay_buffer_size=1000000,
      learning_rate=1e-4,
      learning_rate_decay_steps=10000,
      learning_rate_decay_rate=0.8,
      num_episodes=5000,
      batch_size=64,
      learning_frequency=4,
      update_frequency=20,
      grad_clipping=10.0,
      gamma=0.9,
      double_q=True,
      num_bootstrap_heads=12,
      prioritized=False,
      prioritized_alpha=0.6,
      prioritized_beta=0.4,
      prioritized_epsilon=1e-6,
      fingerprint_radius=3,
      fingerprint_length=2048,
      dense_layers=[1024, 512, 128, 32],
      activation='relu',
      optimizer='Adam',
      batch_norm=False,
      save_frequency=1000,
      max_num_checkpoints=100,
      discount_factor=0.7)
```



#### env

#### **action**

```python
valid_actions = list(environment.get_valid_actions())
```

设为所有可进行的action集合（包括原子添加、键的添加和键的移除）使用**SMILES**

```python
['CCNOON=C=NONN(CC=N)NO', 'CNOON=C=NONN(CC=N)N(C)O', 'C=CNOON=C=NONN(CC=N)NO', 'CNOON=C=NONN(CC=NO)NO', 'CNOON=C=NON1N=CCN1NO', 'CNOON=C=NONN1NOC1C=N', 'CNOON=C=NONN(NO)C(=N)C=N', 'CNOON=C=NOn1n(O)n1CC=N', 'CNOON=C=NONN(CC(=N)N)NO', 'N#CNOON=C=NONN(CC=N)NO', 'CN(N)OON=C=NONN(CC=N)NO', 'CNOON=C=NON1C(=N)CN1NO', 'CNOON=C=NONN(CC=NN)NO', 'N=CCN(NO)NON=C=NOONCN', 'CNOON=C=NONN1C(C=N)N1O', 'C=C(C=N)N(NO)NON=C=NOONC', 'CNOON=C=NONN1CC=NON1', 'C=NOON=C=NONN(CC=N)NO', 'N=CCN(NO)NON=C=NOONCO', 'CNOON=C=NONN1CC(=N)N1O', 'CNOON=C=NON1C(C=N)N1NO', 'CNOONC=NONN(CC=N)NO', 'CNOON=C=NONN(CC(=N)O)NO', 'CNOON=C=NONN(CC=N)N=O', 'CNOON=C=NON(N)N(CC=N)NO', 'N=CCN(NO)NON=C=NOON', 'CNOON=C=NONN(CC(C)=N)NO', 'CNOON=C=NONN(NO)C(O)C=N', 'CNOON=C=NONN(C=C=N)NO', 'CNOON=C=NONN1CC(=N)ON1', 'CNOON=C=NONN(CC=N)NON', 'CNOON=C=NONN(CC#N)NO', 'CNOON=C=NONN(CC=N)NO', 'CN(O)OON=C=NONN(CC=N)NO', 'CNOON=C=NONN(CC=N)NOC', 'CNOON=C=NONN(NO)C(C)C=N', 'C#CNOON=C=NONN(CC=N)NO', 'CNOON=CNONN(CC=N)NO', 'CNOON=C=NONN(NO)C(N)C=N', 'CCN(NO)NON=C=NOONC', 'CNOON=C=NONN(CC=N)N(N)O', 'CNOON=C=NON1ONN1CC=N', 'CNOON=C=NONN(CC=N)N(O)O', 'CNOON=C=NONN(CC=N)NOO', 'N=CCN(NO)NON=C=NOONC=N', 'N=CCN(NO)NON=C=NOONC=O', 'CNOON=C=NONN(NO)C(=O)C=N', 'CN=CCN(NO)NON=C=NOONC', 'CNOON=C=NON(O)N(CC=N)NO', 'CNOON=C=NON(C)N(CC=N)NO', 'CNOON=C=NONN(NO)C1C=N1', 'CNOON=C=NONN1CC=NN1O', 'CNOON=C=NONN(CCN)NO', 'CN(C)OON=C=NONN(CC=N)NO', 'CNOON=C=NONN(N)CC=N']
```

#### state

```python
observations = np.vstack([
    np.append(deep_q_networks.get_fingerprint(act, hparams), steps_left)
    for act in valid_actions
])
```

将可进行的动作SMILES转化为信息指纹(`rdkit`自带）：

```python
fingerprint = AllChem.GetMorganFingerprintAsBitVect(
    molecule, hparams.fingerprint_radius, hparams.fingerprint_length)
```

**shape**:[valid_actions.size, embedding_size]

#### model

全连接层：

```
dense_layers=[1024, 512, 128, 32],
      activation='relu',
      optimizer='Adam',
```

**input_shape** : [valid_actions.size, embedding_size]

**output_shape** : [valid_actions.size， 1]

选择action：tf.max(output)

对确定性模型以及确定型环境，有：

![image-20220125211515782](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220125211515782.png)

对每个action求经NN的值，可相当于$V_{\pi}(S_t)$

从中选出q值最高的action作为选择的action



#### reward

![image-20220125211758392](https://gitee.com/ftfwjft/images/raw/master/image/cloud/image-20220125211758392.png)

其中logP(m), SIM(m, m0)可调用`rdkit`包计算





**代码**

[google-research/mol_dqn at master · google-research/google-research (github.com)](https://github.com/google-research/google-research/tree/master/mol_dqn)

